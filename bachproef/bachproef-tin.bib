% Encoding: UTF-8

@Article{Kaelbling1996,
  author    = {Leslie Pack Kaelbling and Michael L. Littman and Andrew W. Moore},
  title     = {Reinforcement Learning: {A} Survey},
  journal   = {CoRR},
  year      = {1996},
  volume    = {cs.AI/9605103},
  url       = {https://arxiv.org/abs/cs/9605103},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/cs-AI-9605103.bib},
  timestamp = {Fri, 10 Jan 2020 12:58:17 +0100},
}

@Book{Norvig1994,
  author = {Peter Norvig, Stuart J. Russell},
  title  = {Artificial Intelligence: A Modern Approach},
  year   = {1994},
  date   = {1994-12-11},
}

@Book{Hinton1999,
  author    = {Hinton, Geoffrey and Sejnowski, Terrence J.},
  title     = {{Unsupervised Learning: Foundations of Neural Computation}},
  year      = {1999},
  publisher = {The MIT Press},
  isbn      = {9780262288033},
  doi       = {10.7551/mitpress/7011.001.0001},
  url       = {https://doi.org/10.7551/mitpress/7011.001.0001},
  abstract  = {{Since its founding in 1989 by Terrence Sejnowski, Neural Computation has become the leading journal in the field. Foundations of Neural Computation collects, by topic, the most significant papers that have appeared in the journal over the past nine years. This volume of Foundations of Neural Computation, on unsupervised learning algorithms, focuses on neural network learning algorithms that do not require an explicit teacher. The goal of unsupervised learning is to extract an efficient internal representation of the statistical structure implicit in the inputs. These algorithms provide insights into the development of the cerebral cortex and implicit learning in humans. They are also of interest to engineers working in areas such as computer vision and speech recognition who seek efficient representations of raw input data.}},
  month     = {05},
}

@InProceedings{jin2019,
  author       = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
  title        = {Auto-Keras: An Efficient Neural Architecture Search System},
  booktitle    = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year         = {2019},
  organization = {ACM},
  pages        = {1946--1956},
}

@WWW{Gutierrez2019,
  author  = {Daniel Gutierrez},
  title   = {Automated Machine Learning: Myth Versus Reality},
  year    = {2019},
  date    = {2019-01-31},
  url     = {https://opendatascience.com/automated-machine-learning-myth-versus-realty/},
  urldate = {2020-02-24},
}

@InCollection{Feurer2015,
  author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  title     = {Efficient and Robust Automated Machine Learning},
  booktitle = {Advances in Neural Information Processing Systems 28},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  publisher = {Curran Associates, Inc.},
  pages     = {2962--2970},
  url       = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf},
}

@WWW{Khandelwal2019,
  author  = {Renu Khandelwal},
  title   = {Deep Learning using Transfer Learning},
  year    = {2019},
  date    = {2019-08-29},
  url     = {https://towardsdatascience.com/deep-learning-using-transfer-learning-cfbce1578659},
  urldate = {2020-02-24},
}

@Article{Pan2009,
  author   = {S. J. Pan and Q. Yang},
  title    = {A Survey on Transfer Learning},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  year     = {2009},
  date     = {2009-10-16},
  volume   = {22},
  number   = {10},
  pages    = {1345-1359},
  doi      = {10.1109/TKDE.2009.191},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
}

@WWW{GoogleHT2020,
  author       = {Google},
  title        = {Overview of hyperparameter tuning},
  year         = {2020},
  date         = {2020-02-27},
  url          = {https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview},
  organization = {Google},
  urldate      = {2020-02-29},
}

@WWW{Rosebrock2018,
  author       = {Adrian Rosebrock},
  title        = {Building a simple Keras + deep learning REST API},
  year         = {2018},
  date         = {2018-01-29},
  url          = {https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html},
  organization = {Keras},
  urldate      = {2020-02-29},
}

@WWW{Adrian2018,
  author  = {Adrian Rosebrock},
  title   = {A scalable Keras + deep learning REST API},
  year    = {2018},
  date    = {2018-01-29},
  url     = {https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/},
  urldate = {2020-02-29},
}

@WWW{Arroyo2017,
  author = {Armando Arroyo},
  title  = {Business Intelligence and its relationship with the Big Data, Data Analytics and Data Science},
  year   = {2017},
  date   = {2017-02-13},
  url    = {https://www.linkedin.com/pulse/business-intelligence-its-relationship-big-data-geekstyle},
}

@InCollection{Bergstra2011,
  author    = {James S. Bergstra and Bardenet, R\'{e}mi and Bengio, Yoshua and Bal\'{a}zs K\'{e}gl},
  title     = {Algorithms for Hyper-Parameter Optimization},
  booktitle = {Advances in Neural Information Processing Systems 24},
  year      = {2011},
  editor    = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  pages     = {2546--2554},
  url       = {http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
}

@Comment{jabref-meta: databaseType:biblatex;}
