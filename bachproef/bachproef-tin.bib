% Encoding: UTF-8

@Article{Kaelbling1996,
  author    = {Leslie Pack Kaelbling and Michael L. Littman and Andrew W. Moore},
  title     = {Reinforcement Learning: {A} Survey},
  journal   = {CoRR},
  year      = {1996},
  volume    = {cs.AI/9605103},
  url       = {https://arxiv.org/abs/cs/9605103},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/cs-AI-9605103.bib},
  timestamp = {Fri, 10 Jan 2020 12:58:17 +0100},
}

@Book{Norvig1994,
  author = {Peter Norvig, Stuart J. Russell},
  title  = {Artificial Intelligence: A Modern Approach},
  year   = {1994},
  date   = {1994-12-11},
}

@Book{Hinton1999,
  author    = {Hinton, Geoffrey and Sejnowski, Terrence J.},
  title     = {{Unsupervised Learning: Foundations of Neural Computation}},
  year      = {1999},
  publisher = {The MIT Press},
  isbn      = {9780262288033},
  doi       = {10.7551/mitpress/7011.001.0001},
  url       = {https://doi.org/10.7551/mitpress/7011.001.0001},
  abstract  = {{Since its founding in 1989 by Terrence Sejnowski, Neural Computation has become the leading journal in the field. Foundations of Neural Computation collects, by topic, the most significant papers that have appeared in the journal over the past nine years. This volume of Foundations of Neural Computation, on unsupervised learning algorithms, focuses on neural network learning algorithms that do not require an explicit teacher. The goal of unsupervised learning is to extract an efficient internal representation of the statistical structure implicit in the inputs. These algorithms provide insights into the development of the cerebral cortex and implicit learning in humans. They are also of interest to engineers working in areas such as computer vision and speech recognition who seek efficient representations of raw input data.}},
  month     = {05},
}

@InProceedings{jin2019,
  author       = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
  title        = {Auto-Keras: An Efficient Neural Architecture Search System},
  booktitle    = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year         = {2019},
  organization = {ACM},
  pages        = {1946--1956},
}

@WWW{Gutierrez2019,
  author  = {Daniel Gutierrez},
  title   = {Automated Machine Learning: Myth Versus Reality},
  year    = {2019},
  date    = {2019-01-31},
  url     = {https://opendatascience.com/automated-machine-learning-myth-versus-realty/},
  urldate = {2020-02-24},
}

@InCollection{Feurer2015,
  author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  title     = {Efficient and Robust Automated Machine Learning},
  booktitle = {Advances in Neural Information Processing Systems 28},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  publisher = {Curran Associates, Inc.},
  pages     = {2962--2970},
  url       = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf},
}

@WWW{Khandelwal2019,
  author  = {Renu Khandelwal},
  title   = {Deep Learning using Transfer Learning},
  year    = {2019},
  date    = {2019-08-29},
  url     = {https://towardsdatascience.com/deep-learning-using-transfer-learning-cfbce1578659},
  urldate = {2020-02-24},
}

@Article{Pan2009,
  author   = {S. J. Pan and Q. Yang},
  title    = {A Survey on Transfer Learning},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  year     = {2009},
  date     = {2009-10-16},
  volume   = {22},
  number   = {10},
  pages    = {1345-1359},
  doi      = {10.1109/TKDE.2009.191},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
}

@WWW{GoogleHT2020,
  author       = {Google},
  title        = {Overview of hyperparameter tuning},
  year         = {2020},
  date         = {2020-02-27},
  url          = {https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview},
  organization = {Google},
  urldate      = {2020-02-29},
}

@WWW{Rosebrock2018,
  author       = {Adrian Rosebrock},
  title        = {Building a simple Keras + deep learning REST API},
  year         = {2018},
  date         = {2018-01-29},
  url          = {https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html},
  organization = {Keras},
  urldate      = {2020-02-29},
}

@WWW{Adrian2018,
  author  = {Adrian Rosebrock},
  title   = {A scalable Keras + deep learning REST API},
  year    = {2018},
  date    = {2018-01-29},
  url     = {https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/},
  urldate = {2020-02-29},
}

@WWW{Arroyo2017,
  author = {Armando Arroyo},
  title  = {Business Intelligence and its relationship with the Big Data, Data Analytics and Data Science},
  year   = {2017},
  date   = {2017-02-13},
  url    = {https://www.linkedin.com/pulse/business-intelligence-its-relationship-big-data-geekstyle},
}

@InCollection{Bergstra2011,
  author    = {James S. Bergstra and Bardenet, R\'{e}mi and Bengio, Yoshua and Bal\'{a}zs K\'{e}gl},
  title     = {Algorithms for Hyper-Parameter Optimization},
  booktitle = {Advances in Neural Information Processing Systems 24},
  year      = {2011},
  editor    = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  pages     = {2546--2554},
  url       = {http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
}

@Book{Lemahieu2018,
  author    = {Lemahieu, Wilfried and Broucke, Seppe Vanden and Baesens, Bart},
  title     = {Principles of Database Management: The Practical Guide to Storing, Managing and Analyzing Big and Small Data},
  year      = {2018},
  publisher = {Cambridge University Press},
  isbn      = {1107186129},
  address   = {USA},
}

@InProceedings{Bergstra2013,
  author    = {Bergstra, J. and Yamins, D. and Cox, D. D.},
  title     = {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
  booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
  year      = {2013},
  series    = {ICML’13},
  publisher = {JMLR.org},
  location  = {Atlanta, GA, USA},
  pages     = {I–115–I–123},
  numpages  = {9},
}

@InCollection{Feurer2015,
  author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  title     = {Efficient and Robust Automated Machine Learning},
  booktitle = {Advances in Neural Information Processing Systems 28},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  publisher = {Curran Associates, Inc.},
  pages     = {2962--2970},
  url       = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf},
}

@MastersThesis{Schmid1987,
  author    = {Schmidhuber, Jurgen},
  title     = {Evolutionary Principles in Self-Referential Learning. On Learning now to Learn: The Meta-Meta-Meta...-Hook},
  year      = {1987},
  type      = {Diploma Thesis},
  month     = {14 May},
  url       = {http://www.idsia.ch/~juergen/diploma.html},
  added-at  = {2008-06-19T17:46:40.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/2a96f7c3d42103ab94b13badef5d869f0/brazovayeye},
  interhash = {b1d12416bd2edc34c30961f0ae978d8f},
  intrahash = {a96f7c3d42103ab94b13badef5d869f0},
  keywords  = {EURISKO, PSALM, SALM, algorithm, algorithms, associative brigade, bucket evolution, fractals genetic genetical introsepection, learning, meta, nets, neuronal programming self-reference,},
  school    = {Technische Universitat Munchen, Germany},
  size      = {62 pages},
  timestamp = {2008-06-19T17:51:06.000+0200},
}

@InProceedings{Schmidhuber1993,
  author    = {Schmidhuber, J.},
  title     = {A `Self-Referential' Weight Matrix},
  booktitle = {ICANN '93},
  year      = {1993},
  editor    = {Gielen, Stan and Kappen, Bert},
  publisher = {Springer London},
  isbn      = {978-1-4471-2063-6},
  pages     = {446--450},
  abstract  = {Weight modifications in traditional neural nets are computed by hard-wired algorithms. Without exception, all previous weight change algorithms have many specific limitations. Is it (in principle) possible to overcome limitations of hard-wired algorithms by allowing neural nets to run and improve their own weight change algorithms? This paper constructively demonstrates that the answer (in principle) is `yes'. I derive an initial gradientbased sequence learning algorithm for a `self-referential' recurrent network that can `speak' about its own weight matrix in terms of activations. It uses some of its input and output units for observing its own errors and for explicitly analyzing and modifying its own weight matrix, including those parts of the weight matrix responsible for analyzing and modifying the weight matrix. The result is the first `introspective' neural net with explicit potential control over all of its own adaptive parameters. A disadvantage of the algorithm is its high computational complexity per time step which is independent of the sequence length and equals O(nconnlognconn), where riconn is the number of connections. Another disadvantage is the high number of local minima of the unusually complex error surface. The purpose of this paper, however, is not to come up with the most efficient `introspective' or `self-referential' weight change algorithm, but to show that such algorithms are possible at all.},
  address   = {London},
}

@Article{Opitz1999,
  author     = {Opitz, David and Maclin, Richard},
  title      = {Popular Ensemble Methods: An Empirical Study},
  journal    = {J. Artif. Int. Res.},
  year       = {1999},
  volume     = {11},
  number     = {1},
  month      = jul,
  pages      = {169–198},
  issn       = {1076-9757},
  address    = {El Segundo, CA, USA},
  issue_date = {July 1999},
  numpages   = {30},
  publisher  = {AI Access Foundation},
}

@Book{Decorte2019,
  author = {Decorte, J. and De Vreese, S.},
  title  = {Machine learning with Python},
  year   = {2019},
}

@WWW{Cen2016,
  author  = {Alex Cen},
  title   = {Example Ozone data},
  year    = {2016},
  url     = {https://rpubs.com/mp43ily/90520},
  urldate = {2020-03-08},
}

@Article{Zoph2017,
  author        = {Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},
  title         = {Learning Transferable Architectures for Scalable Image Recognition},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1707.07012},
  eprint        = {1707.07012},
  url           = {http://arxiv.org/abs/1707.07012},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/ZophVSL17.bib},
  timestamp     = {Mon, 13 Aug 2018 16:48:00 +0200},
}

@Article{Cai2017,
  author        = {Han Cai and Tianyao Chen and Weinan Zhang and Yong Yu and Jun Wang},
  title         = {Reinforcement Learning for Architecture Search by Network Transformation},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1707.04873},
  eprint        = {1707.04873},
  url           = {http://arxiv.org/abs/1707.04873},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/CaiCZYW17.bib},
  timestamp     = {Sun, 21 Apr 2019 10:04:41 +0200},
}

@InProceedings{Chen2016,
  author    = {Tianqi Chen and Ian J. Goodfellow and Jonathon Shlens},
  title     = {Net2Net: Accelerating Learning via Knowledge Transfer},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  editor    = {Yoshua Bengio and Yann LeCun},
  url       = {http://arxiv.org/abs/1511.05641},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/ChenGS15.bib},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
}

@Comment{jabref-meta: databaseType:biblatex;}
